{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using colab as the basic hardware platform, So we need to change the colab tensorflow version to 2.1.0 and mount the Google Drive.\n",
    "# ===============================================\n",
    "#### 使用colab作为基础硬件平台，因此我们需要将colab tensorflow版本更改为2.1.0并安装谷歌网盘驱动器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25868,
     "status": "ok",
     "timestamp": 1580304843404,
     "user": {
      "displayName": "Steven XU",
      "photoUrl": "",
      "userId": "17790319253083149797"
     },
     "user_tz": -480
    },
    "id": "RdPosU2By5eb",
    "outputId": "0d4d8243-ebc8-4315-b458-08fc73afc15b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path[0] = '/tensorflow-2.1.0/python3.6'\n",
    "from google.colab import drive\n",
    "drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7p4apgJfyvbG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, losses, optimizers, datasets\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34164,
     "status": "ok",
     "timestamp": 1580304851713,
     "user": {
      "displayName": "Steven XU",
      "photoUrl": "",
      "userId": "17790319253083149797"
     },
     "user_tz": -480
    },
    "id": "6bl7IXpEyvbP",
    "outputId": "894f6b33-cadf-4d56-8660-af7b0dcf94e5"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read cifar100 data from keras.datasets \n",
    "# ===============================================\n",
    "#### 从keras.datasets 中读取cifar100 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38027,
     "status": "ok",
     "timestamp": 1580304855584,
     "user": {
      "displayName": "Steven XU",
      "photoUrl": "",
      "userId": "17790319253083149797"
     },
     "user_tz": -480
    },
    "id": "hPGgLfrwyvbV",
    "outputId": "09178cf6-72d6-44e9-9f01-655318192ac8"
   },
   "outputs": [],
   "source": [
    "(x, y), (x_test, y_test) = datasets.cifar100.load_data()\n",
    "print(x.shape,y.shape,x_test.shape,y_test.shape)\n",
    "print(x.dtype,y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data type and values. Normalize x and embedding y to one_hot format and reshape the y dimension to tf accessable dimension.\n",
    "# ===============================================\n",
    "#### 预处理数据类型和值。将x标准化并将y转换到one_hot格式，并将y维度重塑为tf可访问维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stpfbD43yvba"
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(to_categorical(tf.squeeze(tf.cast(y, dtype=tf.int32), axis=1), num_classes=100), dtype=tf.int32)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQrThG6lyvbd"
   },
   "outputs": [],
   "source": [
    "class VGG16(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.VGG16 = Sequential([\n",
    "            layers.Conv2D(64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Conv2D(64, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.MaxPool2D(pool_size=[2,2], strides=2, padding=\"same\"),\n",
    "\n",
    "            layers.Conv2D(128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Conv2D(128, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPool2D(pool_size=[2,2], strides=2, padding=\"same\"),\n",
    "\n",
    "            layers.Conv2D(256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Conv2D(256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Conv2D(256, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPool2D(pool_size=[2,2], strides=2, padding=\"same\"),\n",
    "\n",
    "            layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPool2D(pool_size=[2,2], strides=2, padding=\"same\"),\n",
    "            \n",
    "            layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Conv2D(512, kernel_size=[3,3], padding=\"same\", activation=tf.nn.relu),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPool2D(pool_size=[2,2], strides=2, padding=\"same\"),     \n",
    "\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(4096, activation=tf.nn.relu),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(4096, activation=tf.nn.relu),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(100, activation=tf.nn.softmax)\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        prediction = self.VGG16(x)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xPPVV0Mwyvbf"
   },
   "outputs": [],
   "source": [
    "def main(x, y, x_test, y_test):\n",
    "    epochs = 1000\n",
    "    model = VGG16()\n",
    "    model.build(input_shape=(None, 32, 32, 3))\n",
    "    model.summary()\n",
    "    save_best = keras.callbacks.ModelCheckpoint('/drive/My Drive/Github/CNN/VGG16_best_model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, min_delta=0, patience=100, mode='auto')\n",
    "    callbacks_list = [early_stop, save_best]\n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "                 loss=losses.categorical_crossentropy,\n",
    "                 metrics=['accuracy'])\n",
    "    x, y = preprocess(x, y)\n",
    "    x_test, y_test = preprocess(x_test, y_test)\n",
    "    history = model.fit(x=x, y=y, epochs=epochs, batch_size=512, validation_data=(x_test, y_test), verbose=1, callbacks=callbacks_list)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27496,
     "status": "error",
     "timestamp": 1580294870785,
     "user": {
      "displayName": "Steven XU",
      "photoUrl": "",
      "userId": "17790319253083149797"
     },
     "user_tz": -480
    },
    "id": "M1Na8W_tyvbh",
    "outputId": "56a0561a-c961-4760-bc92-db9afd548cfe"
   },
   "outputs": [],
   "source": [
    "history = main(x, y, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8O9yU2Fyyvbj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VGG16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

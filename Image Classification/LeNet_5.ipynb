{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "LeNet-5.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnI9eFwloRYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fbd11311-ace2-4807-dca4-42e8b49219f8"
      },
      "source": [
        "import sys\n",
        "sys.path[0] = '/tensorflow-2.1.0/python3.6'\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrXorz_SoRYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Sequential, losses, optimizers, datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NcBmXqFoRYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4faa756-21a9-44b4-8286-258f45ee3486"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI4tC7j9oRYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "351a351c-0920-44d3-dd16-dfe768f10a5d"
      },
      "source": [
        "(x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
        "print(x.shape,y.shape,x_test.shape,y_test.shape)\n",
        "print(x.dtype,y.dtype)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
            "uint8 uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8mmjdJroRYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6igewpM8oRY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "2b8d84f1-ec39-4338-dcdf-31a235926456"
      },
      "source": [
        "%matplotlib inline\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1, 5, i)\n",
        "    im = Image.fromarray(x[i])\n",
        "    plt.imshow(im)\n",
        "    plt.title(f'This image is {y[i]}')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABH4AAADvCAYAAACEwBPsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naFdDuMiikd4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x1440 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCOSQoJSoRY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(x, y):\n",
        "    x = tf.expand_dims(tf.cast(x, dtype=tf.float32) / 255., 3)\n",
        "    y = tf.cast(to_categorical(tf.cast(y, dtype=tf.int32), num_classes=10), dtype=tf.int32)\n",
        "    print(x.shape)\n",
        "    return x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAtPvly9oRY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet5(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.LeNet5 = Sequential([\n",
        "            layers.Conv2D(6, kernel_size=[5,5], padding=\"valid\", activation=tf.nn.relu),\n",
        "            layers.MaxPool2D(pool_size=[2,2], strides=2, padding=\"same\"),\n",
        "            layers.Conv2D(16, kernel_size=[5,5], padding=\"valid\", activation=tf.nn.relu),\n",
        "            layers.MaxPool2D(pool_size=[2,2], strides=2, padding=\"same\"),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(120, activation=tf.nn.relu),\n",
        "            layers.Dropout(rate=0.5),\n",
        "            layers.Dense(84, activation=tf.nn.relu),\n",
        "            layers.Dropout(rate=0.5),\n",
        "            layers.Dense(10, activation=tf.nn.softmax)\n",
        "        ])\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs\n",
        "        prediction = self.LeNet5(x)\n",
        "        return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsTcfGznoRY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(x, y, x_test, y_test):\n",
        "    epochs = 1000\n",
        "    model = LeNet5()\n",
        "    model.build(input_shape=(None, 28, 28, 1))\n",
        "    model.summary()\n",
        "    save_best = keras.callbacks.ModelCheckpoint('/drive/My Drive/Github/CNN/LeNet5_best_model.h5',\\\n",
        "                                               monitor='val_loss', verbose=1, save_best_only=True,\\\n",
        "                                               mode='min')\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, min_delta=0, \\\n",
        "                                              patience=100, mode='auto')\n",
        "    callbacks_list = [early_stop, save_best]\n",
        "    model.compile(optimizer=optimizers.Adam(),\n",
        "                 loss=losses.categorical_crossentropy,\n",
        "                 metrics=['accuracy'])\n",
        "    x, y = preprocess(x, y)\n",
        "    x_test, y_test = preprocess(x_test, y_test)\n",
        "    history = model.fit(x=x, y=y, epochs=epochs, batch_size=512, validation_data=(x_test, y_test),\n",
        "                       verbose=1, callbacks=callbacks_list)\n",
        "    return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-z9YgMRoRY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "853df45f-1d5c-4bed-872e-ccafce94a5b7"
      },
      "source": [
        "history = main(x, y, x_test, y_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"le_net5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential (Sequential)      multiple                  44426     \n",
            "=================================================================\n",
            "Total params: 44,426\n",
            "Trainable params: 44,426\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1000\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 1.2035 - accuracy: 0.5941\n",
            "Epoch 00001: val_loss improved from inf to 0.26652, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 8s 129us/sample - loss: 1.1969 - accuracy: 0.5965 - val_loss: 0.2665 - val_accuracy: 0.9246\n",
            "Epoch 2/1000\n",
            "54784/60000 [==========================>...] - ETA: 0s - loss: 0.4237 - accuracy: 0.8747\n",
            "Epoch 00002: val_loss improved from 0.26652 to 0.14842, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.4161 - accuracy: 0.8767 - val_loss: 0.1484 - val_accuracy: 0.9539\n",
            "Epoch 3/1000\n",
            "59904/60000 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.9197\n",
            "Epoch 00003: val_loss improved from 0.14842 to 0.11473, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2789 - accuracy: 0.9197 - val_loss: 0.1147 - val_accuracy: 0.9660\n",
            "Epoch 4/1000\n",
            "55296/60000 [==========================>...] - ETA: 0s - loss: 0.2212 - accuracy: 0.9380\n",
            "Epoch 00004: val_loss improved from 0.11473 to 0.09393, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.2188 - accuracy: 0.9384 - val_loss: 0.0939 - val_accuracy: 0.9701\n",
            "Epoch 5/1000\n",
            "58880/60000 [============================>.] - ETA: 0s - loss: 0.1824 - accuracy: 0.9499\n",
            "Epoch 00005: val_loss improved from 0.09393 to 0.08272, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1823 - accuracy: 0.9498 - val_loss: 0.0827 - val_accuracy: 0.9746\n",
            "Epoch 6/1000\n",
            "55296/60000 [==========================>...] - ETA: 0s - loss: 0.1662 - accuracy: 0.9541\n",
            "Epoch 00006: val_loss improved from 0.08272 to 0.07128, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1660 - accuracy: 0.9545 - val_loss: 0.0713 - val_accuracy: 0.9780\n",
            "Epoch 7/1000\n",
            "55808/60000 [==========================>...] - ETA: 0s - loss: 0.1475 - accuracy: 0.9598\n",
            "Epoch 00007: val_loss improved from 0.07128 to 0.06274, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1472 - accuracy: 0.9599 - val_loss: 0.0627 - val_accuracy: 0.9810\n",
            "Epoch 8/1000\n",
            "58880/60000 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9641\n",
            "Epoch 00008: val_loss did not improve from 0.06274\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1310 - accuracy: 0.9640 - val_loss: 0.0657 - val_accuracy: 0.9801\n",
            "Epoch 9/1000\n",
            "54272/60000 [==========================>...] - ETA: 0s - loss: 0.1237 - accuracy: 0.9668\n",
            "Epoch 00009: val_loss improved from 0.06274 to 0.05763, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1216 - accuracy: 0.9673 - val_loss: 0.0576 - val_accuracy: 0.9838\n",
            "Epoch 10/1000\n",
            "56320/60000 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9690\n",
            "Epoch 00010: val_loss improved from 0.05763 to 0.05256, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1124 - accuracy: 0.9692 - val_loss: 0.0526 - val_accuracy: 0.9839\n",
            "Epoch 11/1000\n",
            "56320/60000 [===========================>..] - ETA: 0s - loss: 0.1060 - accuracy: 0.9712\n",
            "Epoch 00011: val_loss did not improve from 0.05256\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.1052 - accuracy: 0.9713 - val_loss: 0.0553 - val_accuracy: 0.9840\n",
            "Epoch 12/1000\n",
            "57856/60000 [===========================>..] - ETA: 0s - loss: 0.1019 - accuracy: 0.9727\n",
            "Epoch 00012: val_loss improved from 0.05256 to 0.04896, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.1016 - accuracy: 0.9726 - val_loss: 0.0490 - val_accuracy: 0.9856\n",
            "Epoch 13/1000\n",
            "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0926 - accuracy: 0.9745\n",
            "Epoch 00013: val_loss improved from 0.04896 to 0.04548, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0931 - accuracy: 0.9740 - val_loss: 0.0455 - val_accuracy: 0.9862\n",
            "Epoch 14/1000\n",
            "58880/60000 [============================>.] - ETA: 0s - loss: 0.0895 - accuracy: 0.9752\n",
            "Epoch 00014: val_loss improved from 0.04548 to 0.04406, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0898 - accuracy: 0.9752 - val_loss: 0.0441 - val_accuracy: 0.9873\n",
            "Epoch 15/1000\n",
            "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0868 - accuracy: 0.9770\n",
            "Epoch 00015: val_loss did not improve from 0.04406\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0858 - accuracy: 0.9771 - val_loss: 0.0441 - val_accuracy: 0.9874\n",
            "Epoch 16/1000\n",
            "58368/60000 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9785\n",
            "Epoch 00016: val_loss did not improve from 0.04406\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0826 - accuracy: 0.9786 - val_loss: 0.0445 - val_accuracy: 0.9869\n",
            "Epoch 17/1000\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9791\n",
            "Epoch 00017: val_loss improved from 0.04406 to 0.04393, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.0762 - accuracy: 0.9791 - val_loss: 0.0439 - val_accuracy: 0.9865\n",
            "Epoch 18/1000\n",
            "58368/60000 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9797\n",
            "Epoch 00018: val_loss improved from 0.04393 to 0.04090, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0751 - accuracy: 0.9797 - val_loss: 0.0409 - val_accuracy: 0.9876\n",
            "Epoch 19/1000\n",
            "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0713 - accuracy: 0.9803\n",
            "Epoch 00019: val_loss improved from 0.04090 to 0.03583, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0717 - accuracy: 0.9800 - val_loss: 0.0358 - val_accuracy: 0.9895\n",
            "Epoch 20/1000\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9812\n",
            "Epoch 00020: val_loss did not improve from 0.03583\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0681 - accuracy: 0.9811 - val_loss: 0.0391 - val_accuracy: 0.9886\n",
            "Epoch 21/1000\n",
            "56320/60000 [===========================>..] - ETA: 0s - loss: 0.0660 - accuracy: 0.9821\n",
            "Epoch 00021: val_loss did not improve from 0.03583\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0661 - accuracy: 0.9819 - val_loss: 0.0367 - val_accuracy: 0.9895\n",
            "Epoch 22/1000\n",
            "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0651 - accuracy: 0.9830\n",
            "Epoch 00022: val_loss did not improve from 0.03583\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0650 - accuracy: 0.9831 - val_loss: 0.0362 - val_accuracy: 0.9889\n",
            "Epoch 23/1000\n",
            "56320/60000 [===========================>..] - ETA: 0s - loss: 0.0633 - accuracy: 0.9820\n",
            "Epoch 00023: val_loss did not improve from 0.03583\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0630 - accuracy: 0.9821 - val_loss: 0.0378 - val_accuracy: 0.9895\n",
            "Epoch 24/1000\n",
            "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0601 - accuracy: 0.9828\n",
            "Epoch 00024: val_loss did not improve from 0.03583\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.0596 - accuracy: 0.9830 - val_loss: 0.0364 - val_accuracy: 0.9900\n",
            "Epoch 25/1000\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9842\n",
            "Epoch 00025: val_loss improved from 0.03583 to 0.03263, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0588 - accuracy: 0.9840 - val_loss: 0.0326 - val_accuracy: 0.9900\n",
            "Epoch 26/1000\n",
            "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0559 - accuracy: 0.9844\n",
            "Epoch 00026: val_loss did not improve from 0.03263\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0559 - accuracy: 0.9843 - val_loss: 0.0356 - val_accuracy: 0.9902\n",
            "Epoch 27/1000\n",
            "58368/60000 [============================>.] - ETA: 0s - loss: 0.0552 - accuracy: 0.9850\n",
            "Epoch 00027: val_loss improved from 0.03263 to 0.03231, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0551 - accuracy: 0.9850 - val_loss: 0.0323 - val_accuracy: 0.9903\n",
            "Epoch 28/1000\n",
            "58880/60000 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9851\n",
            "Epoch 00028: val_loss did not improve from 0.03231\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0565 - accuracy: 0.9851 - val_loss: 0.0330 - val_accuracy: 0.9892\n",
            "Epoch 29/1000\n",
            "56320/60000 [===========================>..] - ETA: 0s - loss: 0.0519 - accuracy: 0.9856\n",
            "Epoch 00029: val_loss did not improve from 0.03231\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0519 - accuracy: 0.9858 - val_loss: 0.0351 - val_accuracy: 0.9895\n",
            "Epoch 30/1000\n",
            "58880/60000 [============================>.] - ETA: 0s - loss: 0.0492 - accuracy: 0.9862\n",
            "Epoch 00030: val_loss improved from 0.03231 to 0.02944, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0495 - accuracy: 0.9861 - val_loss: 0.0294 - val_accuracy: 0.9902\n",
            "Epoch 31/1000\n",
            "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0498 - accuracy: 0.9859\n",
            "Epoch 00031: val_loss did not improve from 0.02944\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0496 - accuracy: 0.9861 - val_loss: 0.0299 - val_accuracy: 0.9912\n",
            "Epoch 32/1000\n",
            "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0465 - accuracy: 0.9869\n",
            "Epoch 00032: val_loss did not improve from 0.02944\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0465 - accuracy: 0.9868 - val_loss: 0.0332 - val_accuracy: 0.9902\n",
            "Epoch 33/1000\n",
            "58880/60000 [============================>.] - ETA: 0s - loss: 0.0440 - accuracy: 0.9875\n",
            "Epoch 00033: val_loss did not improve from 0.02944\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0443 - accuracy: 0.9874 - val_loss: 0.0326 - val_accuracy: 0.9902\n",
            "Epoch 34/1000\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 0.0467 - accuracy: 0.9868\n",
            "Epoch 00034: val_loss did not improve from 0.02944\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0466 - accuracy: 0.9868 - val_loss: 0.0314 - val_accuracy: 0.9905\n",
            "Epoch 35/1000\n",
            "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0447 - accuracy: 0.9872\n",
            "Epoch 00035: val_loss did not improve from 0.02944\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0448 - accuracy: 0.9872 - val_loss: 0.0312 - val_accuracy: 0.9917\n",
            "Epoch 36/1000\n",
            "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0422 - accuracy: 0.9881\n",
            "Epoch 00036: val_loss did not improve from 0.02944\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0421 - accuracy: 0.9882 - val_loss: 0.0301 - val_accuracy: 0.9906\n",
            "Epoch 37/1000\n",
            "56320/60000 [===========================>..] - ETA: 0s - loss: 0.0430 - accuracy: 0.9882\n",
            "Epoch 00037: val_loss did not improve from 0.02944\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0431 - accuracy: 0.9881 - val_loss: 0.0320 - val_accuracy: 0.9907\n",
            "Epoch 38/1000\n",
            "58880/60000 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9885\n",
            "Epoch 00038: val_loss did not improve from 0.02944\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0402 - accuracy: 0.9885 - val_loss: 0.0315 - val_accuracy: 0.9905\n",
            "Epoch 39/1000\n",
            "59904/60000 [============================>.] - ETA: 0s - loss: 0.0376 - accuracy: 0.9894\n",
            "Epoch 00039: val_loss did not improve from 0.02944\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0375 - accuracy: 0.9894 - val_loss: 0.0308 - val_accuracy: 0.9910\n",
            "Epoch 40/1000\n",
            "58368/60000 [============================>.] - ETA: 0s - loss: 0.0381 - accuracy: 0.9892\n",
            "Epoch 00040: val_loss improved from 0.02944 to 0.02908, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.0380 - accuracy: 0.9892 - val_loss: 0.0291 - val_accuracy: 0.9915\n",
            "Epoch 41/1000\n",
            "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0370 - accuracy: 0.9891\n",
            "Epoch 00041: val_loss improved from 0.02908 to 0.02904, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0366 - accuracy: 0.9891 - val_loss: 0.0290 - val_accuracy: 0.9920\n",
            "Epoch 42/1000\n",
            "58368/60000 [============================>.] - ETA: 0s - loss: 0.0348 - accuracy: 0.9900\n",
            "Epoch 00042: val_loss did not improve from 0.02904\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0350 - accuracy: 0.9899 - val_loss: 0.0305 - val_accuracy: 0.9917\n",
            "Epoch 43/1000\n",
            "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0356 - accuracy: 0.9897\n",
            "Epoch 00043: val_loss did not improve from 0.02904\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0355 - accuracy: 0.9896 - val_loss: 0.0321 - val_accuracy: 0.9913\n",
            "Epoch 44/1000\n",
            "59904/60000 [============================>.] - ETA: 0s - loss: 0.0359 - accuracy: 0.9898\n",
            "Epoch 00044: val_loss did not improve from 0.02904\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0358 - accuracy: 0.9898 - val_loss: 0.0319 - val_accuracy: 0.9911\n",
            "Epoch 45/1000\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9902\n",
            "Epoch 00045: val_loss did not improve from 0.02904\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0339 - accuracy: 0.9901 - val_loss: 0.0315 - val_accuracy: 0.9905\n",
            "Epoch 46/1000\n",
            "58368/60000 [============================>.] - ETA: 0s - loss: 0.0338 - accuracy: 0.9904\n",
            "Epoch 00046: val_loss did not improve from 0.02904\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0337 - accuracy: 0.9904 - val_loss: 0.0332 - val_accuracy: 0.9914\n",
            "Epoch 47/1000\n",
            "58880/60000 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9907\n",
            "Epoch 00047: val_loss improved from 0.02904 to 0.02844, saving model to /drive/My Drive/Github/CNN/LeNet5_best_model.h5\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0320 - accuracy: 0.9908 - val_loss: 0.0284 - val_accuracy: 0.9915\n",
            "Epoch 48/1000\n",
            "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0334 - accuracy: 0.9903\n",
            "Epoch 00048: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0336 - accuracy: 0.9902 - val_loss: 0.0314 - val_accuracy: 0.9914\n",
            "Epoch 49/1000\n",
            "55808/60000 [==========================>...] - ETA: 0s - loss: 0.0312 - accuracy: 0.9910\n",
            "Epoch 00049: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0319 - accuracy: 0.9908 - val_loss: 0.0343 - val_accuracy: 0.9903\n",
            "Epoch 50/1000\n",
            "58368/60000 [============================>.] - ETA: 0s - loss: 0.0306 - accuracy: 0.9912\n",
            "Epoch 00050: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0308 - accuracy: 0.9911 - val_loss: 0.0292 - val_accuracy: 0.9917\n",
            "Epoch 51/1000\n",
            "56832/60000 [===========================>..] - ETA: 0s - loss: 0.0290 - accuracy: 0.9914\n",
            "Epoch 00051: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0295 - accuracy: 0.9913 - val_loss: 0.0324 - val_accuracy: 0.9917\n",
            "Epoch 52/1000\n",
            "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0297 - accuracy: 0.9910\n",
            "Epoch 00052: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0299 - accuracy: 0.9910 - val_loss: 0.0360 - val_accuracy: 0.9902\n",
            "Epoch 53/1000\n",
            "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0301 - accuracy: 0.9908\n",
            "Epoch 00053: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0296 - accuracy: 0.9909 - val_loss: 0.0332 - val_accuracy: 0.9915\n",
            "Epoch 54/1000\n",
            "54784/60000 [==========================>...] - ETA: 0s - loss: 0.0305 - accuracy: 0.9913\n",
            "Epoch 00054: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0304 - accuracy: 0.9914 - val_loss: 0.0320 - val_accuracy: 0.9909\n",
            "Epoch 55/1000\n",
            "57344/60000 [===========================>..] - ETA: 0s - loss: 0.0283 - accuracy: 0.9921\n",
            "Epoch 00055: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0285 - accuracy: 0.9919 - val_loss: 0.0340 - val_accuracy: 0.9906\n",
            "Epoch 56/1000\n",
            "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0274 - accuracy: 0.9915\n",
            "Epoch 00056: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.0327 - val_accuracy: 0.9902\n",
            "Epoch 57/1000\n",
            "56320/60000 [===========================>..] - ETA: 0s - loss: 0.0254 - accuracy: 0.9922\n",
            "Epoch 00057: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0253 - accuracy: 0.9922 - val_loss: 0.0343 - val_accuracy: 0.9911\n",
            "Epoch 58/1000\n",
            "54272/60000 [==========================>...] - ETA: 0s - loss: 0.0279 - accuracy: 0.9917\n",
            "Epoch 00058: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0280 - accuracy: 0.9917 - val_loss: 0.0318 - val_accuracy: 0.9919\n",
            "Epoch 59/1000\n",
            "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0272 - accuracy: 0.9921\n",
            "Epoch 00059: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0272 - accuracy: 0.9921 - val_loss: 0.0339 - val_accuracy: 0.9910\n",
            "Epoch 60/1000\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 0.0270 - accuracy: 0.9924\n",
            "Epoch 00060: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0269 - accuracy: 0.9924 - val_loss: 0.0356 - val_accuracy: 0.9911\n",
            "Epoch 61/1000\n",
            "55296/60000 [==========================>...] - ETA: 0s - loss: 0.0252 - accuracy: 0.9928\n",
            "Epoch 00061: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0255 - accuracy: 0.9927 - val_loss: 0.0343 - val_accuracy: 0.9909\n",
            "Epoch 62/1000\n",
            "57856/60000 [===========================>..] - ETA: 0s - loss: 0.0249 - accuracy: 0.9922\n",
            "Epoch 00062: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0254 - accuracy: 0.9922 - val_loss: 0.0350 - val_accuracy: 0.9896\n",
            "Epoch 63/1000\n",
            "59392/60000 [============================>.] - ETA: 0s - loss: 0.0260 - accuracy: 0.9926\n",
            "Epoch 00063: val_loss did not improve from 0.02844\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0261 - accuracy: 0.9926 - val_loss: 0.0308 - val_accuracy: 0.9917\n",
            "Epoch 64/1000\n",
            "27136/60000 [============>.................] - ETA: 0s - loss: 0.0237 - accuracy: 0.9936WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-618ab43cea25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-feeda9d6f54a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(x, y, x_test, y_test)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     history = model.fit(x=x, y=y, epochs=epochs, batch_size=512, validation_data=(x_test, y_test),\n\u001b[0;32m---> 15\u001b[0;31m                        verbose=1, callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUc3Qoh8oRY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc3mWiS-oRZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
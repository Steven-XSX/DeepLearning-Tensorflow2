{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "InceptionV1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIIoBni_kmaZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "89b86d1a-b374-46e7-ff88-1715127a80e2"
      },
      "source": [
        "import sys\n",
        "sys.path[0] = '/tensorflow-2.1.0/python3.6'\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAKqOq1wkmaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Sequential, losses, optimizers, datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opngaWQ9kman",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6915222b-c077-4d3d-980f-d35b94e3ebe6"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_W_QZttkmar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oku33oVykmau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RTG9rvdkmaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(x, y):\n",
        "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
        "    y = tf.cast(to_categorical(tf.cast(y, dtype=tf.int32), num_classes=100), dtype=tf.int32)\n",
        "    return x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb4kDVnwmubL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "76d7cd06-3ad0-4c58-eb14-becc191ad960"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/drive/My Drive/Github/CNN/data/train',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=128,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        '/drive/My Drive/Github/CNN/data/test',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=128,\n",
        "        class_mode='categorical')\n",
        "\n",
        "def my_generator(generator):\n",
        "    while True:\n",
        "        data = next(generator)\n",
        "        x = data[0]\n",
        "        y = data[1] , data[1], data[1]\n",
        "        yield x, y\n",
        "\n",
        "final_generator = my_generator(train_generator)\n",
        "val_generator = my_generator(validation_generator)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2500 images belonging to 5 classes.\n",
            "Found 1500 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyNXIpHwkmay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %matplotlib inline\n",
        "# plt.figure(figsize=(20,20))\n",
        "# for i in range(1, 6):\n",
        "#     plt.subplot(1, 5, i)\n",
        "#     im = Image.fromarray(x[i])\n",
        "#     im = im.resize((224, 224), Image.ANTIALIAS)\n",
        "#     plt.imshow(im)\n",
        "#     plt.title(f'This image is {y[i]}')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwGgYhUVkma0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class Inception(layers.Layer):\n",
        "        def __init__(self, filters):\n",
        "            super(Inception, self).__init__()\n",
        "            self.path1 = layers.Conv2D(filters[0][0], kernel_size=(1, 1), strides=1, padding='same',\\\n",
        "                              activation='relu')\n",
        "            self.path2_1 = layers.Conv2D(filters[1][0], kernel_size=(1, 1), strides=1, padding='same',\\\n",
        "                              activation='relu')\n",
        "            self.path2_2 = layers.Conv2D(filters[1][1], kernel_size=(3, 3), strides=1, padding='same',\\\n",
        "                              activation='relu')\n",
        "            self.path3_1 = layers.Conv2D(filters[2][0], kernel_size=(1, 1), strides=1, padding='same',\\\n",
        "                              activation='relu')\n",
        "            self.path3_2 = layers.Conv2D(filters[2][1], kernel_size=(5, 5), strides=1, padding='same',\\\n",
        "                              activation='relu')\n",
        "            \n",
        "            self.path4_1 = layers.MaxPool2D(pool_size=(3, 3), strides=1, padding='same')\n",
        "            self.path4_2 = layers.Conv2D(filters[3][0], kernel_size=(1, 1), strides=1, padding='same',\\\n",
        "                             activation='relu')\n",
        "        \n",
        "        def call(self, inputs, training=None):\n",
        "            out1 = self.path1(inputs)\n",
        "            out2 = self.path2_1(inputs)\n",
        "            out2 = self.path2_2(out2)\n",
        "            out3 = self.path3_1(inputs)\n",
        "            out3 = self.path3_2(out3)\n",
        "            out4 = self.path4_1(inputs)\n",
        "            out4 = self.path4_2(out4)\n",
        "            return layers.Concatenate(axis=-1)([out1, out2, out3, out4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRjlsv35kma2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Auxiliary(layers.Layer):\n",
        "    def __init__(self, filters=[128, 1024, 5]):\n",
        "        super(Auxiliary, self).__init__()\n",
        "        self.avgpool = layers.AvgPool2D(pool_size=(5, 5), strides=3, padding='valid')\n",
        "        self.conv1 = layers.Conv2D(filters[0], kernel_size=(1, 1), strides=1, padding='same',\\\n",
        "                            activation='relu')\n",
        "        self.flatten1 = layers.Flatten()\n",
        "        self.dense1 = layers.Dense(filters[1], activation='relu')\n",
        "        self.drop = layers.Dropout(0.7)\n",
        "        self.dense2 = layers.Dense(filters[2], activation='softmax')\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        out = self.avgpool(inputs)\n",
        "        out = self.conv1(out)\n",
        "        out = self.flatten1(out)\n",
        "        out = self.dense1(out)\n",
        "        out = self.drop(out)\n",
        "        out = self.dense2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVXRFmnfkma3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InceptionV1(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(InceptionV1, self).__init__()\n",
        "        self.stem = Sequential([\n",
        "            layers.Conv2D(64, kernel_size=(7, 7), strides=2, padding='same', activation='relu'),\n",
        "            layers.MaxPool2D(pool_size=(3, 3), strides=2, padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            \n",
        "            layers.Conv2D(64, kernel_size=(1, 1), strides=1, padding='same', activation='relu'),\n",
        "            layers.Conv2D(192, kernel_size=(3, 3), strides=1, padding='same', activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPool2D(pool_size=(3, 3), strides=2, padding='same'),\n",
        "            \n",
        "            #inception3\n",
        "            Inception(filters=[[64], [96,128], [16, 32], [32]]),\n",
        "            Inception(filters=[[128], [128,192], [32, 96], [64]]),\n",
        "            layers.MaxPool2D(pool_size=(3, 3), strides=2, padding='same')\n",
        "            ])\n",
        "            \n",
        "        #inception4\n",
        "        self.inception4a = Inception(filters=[[192], [96,208], [16, 48], [64]])\n",
        "        self.inception4b = Inception(filters=[[160], [112,224], [24, 64], [64]])\n",
        "        self.auxiliary1 = Auxiliary()\n",
        "        self.inception4c = Inception(filters=[[128], [128,256], [24, 64], [64]])\n",
        "        self.inception4d = Inception(filters=[[112], [144,288], [32, 64], [64]])\n",
        "        self.inception4e = Inception(filters=[[256], [160,320], [32, 128], [128]])\n",
        "        self.auxiliary2 = Auxiliary()\n",
        "        self.maxpool1 = layers.MaxPool2D(pool_size=(3, 3), strides=2, padding='same')\n",
        "        #inception5\n",
        "        self.inception5a = Inception(filters=[[256], [160,320], [32, 128], [128]])\n",
        "        self.inception5b = Inception(filters=[[384], [192,384], [48, 128], [128]])\n",
        "        self.avgpool1 = layers.AvgPool2D(pool_size=(7, 7), strides=1, padding='valid')\n",
        "        self.flatten1 = layers.Flatten()\n",
        "        self.drop1 = layers.Dropout(0.4)\n",
        "        self.dense1 = layers.Dense(256, activation='linear')\n",
        "        self.dense2 = layers.Dense(5, activation='softmax')\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        x = self.stem(inputs)\n",
        "        x = self.inception4a(x)\n",
        "        a1 = self.auxiliary1(x)\n",
        "        x = self.inception4b(x)\n",
        "        x = self.inception4c(x)\n",
        "        x = self.inception4d(x)\n",
        "        a2 = self.auxiliary2(x)\n",
        "        x = self.inception4e(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.inception5a(x)\n",
        "        x = self.inception5b(x)\n",
        "        x = self.avgpool1(x)\n",
        "        x = self.flatten1(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return [x, a1, a2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJWkVCWSkma5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee220d0c-d5c9-49ed-d757-ed8af0121fac"
      },
      "source": [
        "tf.compat.v1.disable_v2_behavior()\n",
        "epochs = 1000\n",
        "model = InceptionV1()\n",
        "model.build(input_shape=(None, 224, 224, 3))\n",
        "model.summary()\n",
        "optimizer = optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
        "save_best = keras.callbacks.ModelCheckpoint('/drive/My Drive/Github/CNN/InceptionV1_best_model.h5',\\\n",
        "                                               monitor='val_loss', verbose=1, save_best_only=True,\\\n",
        "                                               mode='min')\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, min_delta=0, \\\n",
        "                                              patience=100, mode='auto')\n",
        "def scheduler(epoch):\n",
        "    if epoch < 8:\n",
        "        return 0.1\n",
        "    else:\n",
        "        return 0.1 * math.pow(0.96, epoch // 8)\n",
        "learning_rate = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
        "callbacks_list = [save_best, early_stop, learning_rate]\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=[losses.categorical_crossentropy, losses.categorical_crossentropy, losses.categorical_crossentropy],\n",
        "              loss_weights={'output_1': 1.0, 'output_2': 0.3, 'output_3': 0.3},\n",
        "             metrics=['accuracy'])\n",
        "history = model.fit(final_generator, epochs=epochs, validation_data=val_generator, steps_per_epoch=6, validation_steps=2,\n",
        "                       verbose=1, callbacks=callbacks_list, use_multiprocessing=True)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v1_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_30 (Sequential)   multiple                  677872    \n",
            "_________________________________________________________________\n",
            "inception_272 (Inception)    multiple                  376176    \n",
            "_________________________________________________________________\n",
            "inception_273 (Inception)    multiple                  449160    \n",
            "_________________________________________________________________\n",
            "auxiliary_60 (Auxiliary)     multiple                  2168965   \n",
            "_________________________________________________________________\n",
            "inception_274 (Inception)    multiple                  510104    \n",
            "_________________________________________________________________\n",
            "inception_275 (Inception)    multiple                  605376    \n",
            "_________________________________________________________________\n",
            "inception_276 (Inception)    multiple                  868352    \n",
            "_________________________________________________________________\n",
            "auxiliary_61 (Auxiliary)     multiple                  2171013   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_400 (MaxPoolin multiple                  0         \n",
            "_________________________________________________________________\n",
            "inception_277 (Inception)    multiple                  1043456   \n",
            "_________________________________________________________________\n",
            "inception_278 (Inception)    multiple                  1444080   \n",
            "_________________________________________________________________\n",
            "average_pooling2d_92 (Averag multiple                  0         \n",
            "_________________________________________________________________\n",
            "flatten_92 (Flatten)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dropout_92 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_184 (Dense)            multiple                  262400    \n",
            "_________________________________________________________________\n",
            "dense_185 (Dense)            multiple                  1285      \n",
            "=================================================================\n",
            "Total params: 10,578,239\n",
            "Trainable params: 10,577,727\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
            "Epoch 1/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.8413 - output_1_loss: 1.6416 - output_2_loss: 2.6260 - output_3_loss: 1.7557 - output_1_acc: 0.2095 - output_2_acc: 0.1947 - output_3_acc: 0.1774WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 5s - loss: 2.5808 - output_1_loss: 1.6112 - output_2_loss: 1.6251 - output_3_loss: 1.6071 - output_1_acc: 0.2031 - output_2_acc: 0.2539 - output_3_acc: 0.2266 \n",
            "Epoch 00001: val_loss improved from inf to 2.58084, saving model to /drive/My Drive/Github/CNN/InceptionV1_best_model.h5\n",
            "6/6 [==============================] - 34s 6s/step - loss: 2.7944 - output_1_loss: 1.6386 - output_2_loss: 2.5060 - output_3_loss: 1.7321 - output_1_acc: 0.2081 - output_2_acc: 0.2001 - output_3_acc: 0.1791 - val_loss: 2.5808 - val_output_1_loss: 1.6112 - val_output_2_loss: 1.6251 - val_output_3_loss: 1.6071 - val_output_1_acc: 0.2031 - val_output_2_acc: 0.2539 - val_output_3_acc: 0.2266\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
            "Epoch 2/1000\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 2.5383 - output_1_loss: 1.6016 - output_2_loss: 1.5739 - output_3_loss: 1.5975 - output_1_acc: 0.1964 - output_2_acc: 0.3024 - output_3_acc: 0.2287WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 3s - loss: 2.6818 - output_1_loss: 1.6478 - output_2_loss: 1.8436 - output_3_loss: 1.6030 - output_1_acc: 0.1641 - output_2_acc: 0.2188 - output_3_acc: 0.2188\n",
            "Epoch 00002: val_loss did not improve from 2.58084\n",
            "6/6 [==============================] - 4s 646ms/step - loss: 2.5295 - output_1_loss: 1.5991 - output_2_loss: 1.5615 - output_3_loss: 1.5959 - output_1_acc: 0.2002 - output_2_acc: 0.3120 - output_3_acc: 0.2292 - val_loss: 2.6818 - val_output_1_loss: 1.6478 - val_output_2_loss: 1.8436 - val_output_3_loss: 1.6030 - val_output_1_acc: 0.1641 - val_output_2_acc: 0.2188 - val_output_3_acc: 0.2188\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
            "Epoch 3/1000\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 2.3945 - output_1_loss: 1.5587 - output_2_loss: 1.4587 - output_3_loss: 1.5405 - output_1_acc: 0.3060 - output_2_acc: 0.3541 - output_3_acc: 0.2991WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.8578 - output_1_loss: 1.7832 - output_2_loss: 1.9492 - output_3_loss: 1.6327 - output_1_acc: 0.1641 - output_2_acc: 0.2031 - output_3_acc: 0.1641\n",
            "Epoch 00003: val_loss did not improve from 2.58084\n",
            "6/6 [==============================] - 5s 785ms/step - loss: 2.4105 - output_1_loss: 1.5529 - output_2_loss: 1.4454 - output_3_loss: 1.5275 - output_1_acc: 0.3056 - output_2_acc: 0.3686 - output_3_acc: 0.3074 - val_loss: 2.8578 - val_output_1_loss: 1.7832 - val_output_2_loss: 1.9492 - val_output_3_loss: 1.6327 - val_output_1_acc: 0.1641 - val_output_2_acc: 0.2031 - val_output_3_acc: 0.1641\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
            "Epoch 4/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.6233 - output_1_loss: 1.7076 - output_2_loss: 1.6425 - output_3_loss: 1.6031 - output_1_acc: 0.2035 - output_2_acc: 0.2721 - output_3_acc: 0.2294WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.6123 - output_1_loss: 1.6424 - output_2_loss: 1.6020 - output_3_loss: 1.6310 - output_1_acc: 0.1641 - output_2_acc: 0.2656 - output_3_acc: 0.1641\n",
            "Epoch 00004: val_loss did not improve from 2.58084\n",
            "6/6 [==============================] - 11s 2s/step - loss: 2.6068 - output_1_loss: 1.6939 - output_2_loss: 1.6224 - output_3_loss: 1.5992 - output_1_acc: 0.2047 - output_2_acc: 0.2803 - output_3_acc: 0.2333 - val_loss: 2.6123 - val_output_1_loss: 1.6424 - val_output_2_loss: 1.6020 - val_output_3_loss: 1.6310 - val_output_1_acc: 0.1641 - val_output_2_acc: 0.2656 - val_output_3_acc: 0.1641\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
            "Epoch 5/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.4713 - output_1_loss: 1.6026 - output_2_loss: 1.4755 - output_3_loss: 1.5604 - output_1_acc: 0.2271 - output_2_acc: 0.3756 - output_3_acc: 0.2833WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 3.4700 - output_1_loss: 1.8618 - output_2_loss: 3.2898 - output_3_loss: 2.0709 - output_1_acc: 0.2539 - output_2_acc: 0.2539 - output_3_acc: 0.2578\n",
            "Epoch 00005: val_loss did not improve from 2.58084\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.4689 - output_1_loss: 1.5982 - output_2_loss: 1.4619 - output_3_loss: 1.5465 - output_1_acc: 0.2318 - output_2_acc: 0.3814 - output_3_acc: 0.2972 - val_loss: 3.4700 - val_output_1_loss: 1.8618 - val_output_2_loss: 3.2898 - val_output_3_loss: 2.0709 - val_output_1_acc: 0.2539 - val_output_2_acc: 0.2539 - val_output_3_acc: 0.2578\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
            "Epoch 6/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.5689 - output_1_loss: 1.6229 - output_2_loss: 1.6676 - output_3_loss: 1.5879 - output_1_acc: 0.2424 - output_2_acc: 0.3042 - output_3_acc: 0.2455WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 3s - loss: 2.6979 - output_1_loss: 1.6483 - output_2_loss: 1.8204 - output_3_loss: 1.6784 - output_1_acc: 0.1680 - output_2_acc: 0.3203 - output_3_acc: 0.2656\n",
            "Epoch 00006: val_loss did not improve from 2.58084\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.5693 - output_1_loss: 1.6221 - output_2_loss: 1.6372 - output_3_loss: 1.5923 - output_1_acc: 0.2394 - output_2_acc: 0.3129 - output_3_acc: 0.2368 - val_loss: 2.6979 - val_output_1_loss: 1.6483 - val_output_2_loss: 1.8204 - val_output_3_loss: 1.6784 - val_output_1_acc: 0.1680 - val_output_2_acc: 0.3203 - val_output_3_acc: 0.2656\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
            "Epoch 7/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.4970 - output_1_loss: 1.6042 - output_2_loss: 1.4523 - output_3_loss: 1.5873 - output_1_acc: 0.2198 - output_2_acc: 0.3573 - output_3_acc: 0.2480WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 3s - loss: 2.7660 - output_1_loss: 1.6876 - output_2_loss: 1.8850 - output_3_loss: 1.7097 - output_1_acc: 0.2109 - output_2_acc: 0.2852 - output_3_acc: 0.2227\n",
            "Epoch 00007: val_loss did not improve from 2.58084\n",
            "6/6 [==============================] - 11s 2s/step - loss: 2.4873 - output_1_loss: 1.6004 - output_2_loss: 1.4406 - output_3_loss: 1.5840 - output_1_acc: 0.2268 - output_2_acc: 0.3658 - output_3_acc: 0.2506 - val_loss: 2.7660 - val_output_1_loss: 1.6876 - val_output_2_loss: 1.8850 - val_output_3_loss: 1.7097 - val_output_1_acc: 0.2109 - val_output_2_acc: 0.2852 - val_output_3_acc: 0.2227\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
            "Epoch 8/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.3488 - output_1_loss: 1.5466 - output_2_loss: 1.2725 - output_3_loss: 1.4963 - output_1_acc: 0.3070 - output_2_acc: 0.4995 - output_3_acc: 0.3428WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.5990 - output_1_loss: 1.6574 - output_2_loss: 1.5467 - output_3_loss: 1.5919 - output_1_acc: 0.2539 - output_2_acc: 0.3203 - output_3_acc: 0.2852\n",
            "Epoch 00008: val_loss did not improve from 2.58084\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.3795 - output_1_loss: 1.5485 - output_2_loss: 1.2703 - output_3_loss: 1.4944 - output_1_acc: 0.3097 - output_2_acc: 0.4959 - output_3_acc: 0.3398 - val_loss: 2.5990 - val_output_1_loss: 1.6574 - val_output_2_loss: 1.5467 - val_output_3_loss: 1.5919 - val_output_1_acc: 0.2539 - val_output_2_acc: 0.3203 - val_output_3_acc: 0.2852\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.096.\n",
            "Epoch 9/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.3573 - output_1_loss: 1.6613 - output_2_loss: 1.1487 - output_3_loss: 1.5250 - output_1_acc: 0.2168 - output_2_acc: 0.5160 - output_3_acc: 0.2724WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.4527 - output_1_loss: 1.5484 - output_2_loss: 1.5562 - output_3_loss: 1.4583 - output_1_acc: 0.3672 - output_2_acc: 0.3477 - output_3_acc: 0.3945\n",
            "Epoch 00009: val_loss improved from 2.58084 to 2.45273, saving model to /drive/My Drive/Github/CNN/InceptionV1_best_model.h5\n",
            "6/6 [==============================] - 13s 2s/step - loss: 2.3057 - output_1_loss: 1.6289 - output_2_loss: 1.1294 - output_3_loss: 1.5021 - output_1_acc: 0.2475 - output_2_acc: 0.5282 - output_3_acc: 0.3010 - val_loss: 2.4527 - val_output_1_loss: 1.5484 - val_output_2_loss: 1.5562 - val_output_3_loss: 1.4583 - val_output_1_acc: 0.3672 - val_output_2_acc: 0.3477 - val_output_3_acc: 0.3945\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.096.\n",
            "Epoch 10/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.3099 - output_1_loss: 1.4647 - output_2_loss: 0.9954 - output_3_loss: 1.3462 - output_1_acc: 0.3639 - output_2_acc: 0.6017 - output_3_acc: 0.4709WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.6651 - output_1_loss: 1.7144 - output_2_loss: 1.5529 - output_3_loss: 1.6163 - output_1_acc: 0.2539 - output_2_acc: 0.3086 - output_3_acc: 0.2539\n",
            "Epoch 00010: val_loss did not improve from 2.45273\n",
            "6/6 [==============================] - 10s 2s/step - loss: 2.6121 - output_1_loss: 1.5488 - output_2_loss: 1.0741 - output_3_loss: 1.4114 - output_1_acc: 0.3556 - output_2_acc: 0.5876 - output_3_acc: 0.4530 - val_loss: 2.6651 - val_output_1_loss: 1.7144 - val_output_2_loss: 1.5529 - val_output_3_loss: 1.6163 - val_output_1_acc: 0.2539 - val_output_2_acc: 0.3086 - val_output_3_acc: 0.2539\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.096.\n",
            "Epoch 11/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.6418 - output_1_loss: 1.7397 - output_2_loss: 1.5116 - output_3_loss: 1.6293 - output_1_acc: 0.1926 - output_2_acc: 0.3443 - output_3_acc: 0.1874WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 3s - loss: 2.5945 - output_1_loss: 1.6342 - output_2_loss: 1.6079 - output_3_loss: 1.5929 - output_1_acc: 0.1641 - output_2_acc: 0.2461 - output_3_acc: 0.2578\n",
            "Epoch 00011: val_loss did not improve from 2.45273\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.6965 - output_1_loss: 1.7219 - output_2_loss: 1.5899 - output_3_loss: 1.6241 - output_1_acc: 0.2057 - output_2_acc: 0.3445 - output_3_acc: 0.1893 - val_loss: 2.5945 - val_output_1_loss: 1.6342 - val_output_2_loss: 1.6079 - val_output_3_loss: 1.5929 - val_output_1_acc: 0.1641 - val_output_2_acc: 0.2461 - val_output_3_acc: 0.2578\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.096.\n",
            "Epoch 12/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.4836 - output_1_loss: 1.5842 - output_2_loss: 1.5833 - output_3_loss: 1.5381 - output_1_acc: 0.2790 - output_2_acc: 0.2861 - output_3_acc: 0.2789WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 3s - loss: 2.8576 - output_1_loss: 1.7735 - output_2_loss: 1.8253 - output_3_loss: 1.7885 - output_1_acc: 0.2539 - output_2_acc: 0.2891 - output_3_acc: 0.2539\n",
            "Epoch 00012: val_loss did not improve from 2.45273\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.4647 - output_1_loss: 1.5746 - output_2_loss: 1.5690 - output_3_loss: 1.5314 - output_1_acc: 0.2845 - output_2_acc: 0.3055 - output_3_acc: 0.2818 - val_loss: 2.8576 - val_output_1_loss: 1.7735 - val_output_2_loss: 1.8253 - val_output_3_loss: 1.7885 - val_output_1_acc: 0.2539 - val_output_2_acc: 0.2891 - val_output_3_acc: 0.2539\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.096.\n",
            "Epoch 13/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.3942 - output_1_loss: 1.5066 - output_2_loss: 1.4208 - output_3_loss: 1.4587 - output_1_acc: 0.3503 - output_2_acc: 0.4405 - output_3_acc: 0.3598WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.6106 - output_1_loss: 1.6646 - output_2_loss: 1.6871 - output_3_loss: 1.4661 - output_1_acc: 0.1836 - output_2_acc: 0.2773 - output_3_acc: 0.4219\n",
            "Epoch 00013: val_loss did not improve from 2.45273\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.4202 - output_1_loss: 1.5200 - output_2_loss: 1.4117 - output_3_loss: 1.4704 - output_1_acc: 0.3351 - output_2_acc: 0.4408 - output_3_acc: 0.3478 - val_loss: 2.6106 - val_output_1_loss: 1.6646 - val_output_2_loss: 1.6871 - val_output_3_loss: 1.4661 - val_output_1_acc: 0.1836 - val_output_2_acc: 0.2773 - val_output_3_acc: 0.4219\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.096.\n",
            "Epoch 14/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.4307 - output_1_loss: 1.5966 - output_2_loss: 1.3380 - output_3_loss: 1.5337 - output_1_acc: 0.1949 - output_2_acc: 0.4419 - output_3_acc: 0.3234WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 5.2937 - output_1_loss: 3.5068 - output_2_loss: 2.1540 - output_3_loss: 3.8024 - output_1_acc: 0.2539 - output_2_acc: 0.2969 - output_3_acc: 0.2695\n",
            "Epoch 00014: val_loss did not improve from 2.45273\n",
            "6/6 [==============================] - 11s 2s/step - loss: 2.3973 - output_1_loss: 1.5837 - output_2_loss: 1.3356 - output_3_loss: 1.5223 - output_1_acc: 0.2070 - output_2_acc: 0.4420 - output_3_acc: 0.3356 - val_loss: 5.2937 - val_output_1_loss: 3.5068 - val_output_2_loss: 2.1540 - val_output_3_loss: 3.8024 - val_output_1_acc: 0.2539 - val_output_2_acc: 0.2969 - val_output_3_acc: 0.2695\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.096.\n",
            "Epoch 15/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.1954 - output_1_loss: 1.4661 - output_2_loss: 1.1604 - output_3_loss: 1.3820 - output_1_acc: 0.2893 - output_2_acc: 0.5622 - output_3_acc: 0.4204WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 6.0270 - output_1_loss: 3.7330 - output_2_loss: 3.7794 - output_3_loss: 3.8671 - output_1_acc: 0.2539 - output_2_acc: 0.2539 - output_3_acc: 0.2539\n",
            "Epoch 00015: val_loss did not improve from 2.45273\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.1740 - output_1_loss: 1.4566 - output_2_loss: 1.1449 - output_3_loss: 1.3770 - output_1_acc: 0.2937 - output_2_acc: 0.5682 - output_3_acc: 0.4167 - val_loss: 6.0270 - val_output_1_loss: 3.7330 - val_output_2_loss: 3.7794 - val_output_3_loss: 3.8671 - val_output_1_acc: 0.2539 - val_output_2_acc: 0.2539 - val_output_3_acc: 0.2539\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.096.\n",
            "Epoch 16/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.2392 - output_1_loss: 1.4945 - output_2_loss: 1.2608 - output_3_loss: 1.4145 - output_1_acc: 0.2871 - output_2_acc: 0.4979 - output_3_acc: 0.3872WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.4293 - output_1_loss: 1.4766 - output_2_loss: 1.5406 - output_3_loss: 1.6350 - output_1_acc: 0.2773 - output_2_acc: 0.2852 - output_3_acc: 0.2734\n",
            "Epoch 00016: val_loss improved from 2.45273 to 2.42926, saving model to /drive/My Drive/Github/CNN/InceptionV1_best_model.h5\n",
            "6/6 [==============================] - 13s 2s/step - loss: 2.2192 - output_1_loss: 1.4816 - output_2_loss: 1.2415 - output_3_loss: 1.4025 - output_1_acc: 0.2951 - output_2_acc: 0.5082 - output_3_acc: 0.3948 - val_loss: 2.4293 - val_output_1_loss: 1.4766 - val_output_2_loss: 1.5406 - val_output_3_loss: 1.6350 - val_output_1_acc: 0.2773 - val_output_2_acc: 0.2852 - val_output_3_acc: 0.2734\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.09216.\n",
            "Epoch 17/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.1964 - output_1_loss: 1.4562 - output_2_loss: 1.0864 - output_3_loss: 1.3310 - output_1_acc: 0.3318 - output_2_acc: 0.5820 - output_3_acc: 0.4340WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 4.6377 - output_1_loss: 3.1583 - output_2_loss: 2.3003 - output_3_loss: 2.6312 - output_1_acc: 0.2539 - output_2_acc: 0.2656 - output_3_acc: 0.2695\n",
            "Epoch 00017: val_loss did not improve from 2.42926\n",
            "6/6 [==============================] - 10s 2s/step - loss: 2.1538 - output_1_loss: 1.4529 - output_2_loss: 1.0839 - output_3_loss: 1.3198 - output_1_acc: 0.3371 - output_2_acc: 0.5828 - output_3_acc: 0.4484 - val_loss: 4.6377 - val_output_1_loss: 3.1583 - val_output_2_loss: 2.3003 - val_output_3_loss: 2.6312 - val_output_1_acc: 0.2539 - val_output_2_acc: 0.2656 - val_output_3_acc: 0.2695\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.09216.\n",
            "Epoch 18/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.9385 - output_1_loss: 1.3356 - output_2_loss: 0.9218 - output_3_loss: 1.1254 - output_1_acc: 0.4514 - output_2_acc: 0.6265 - output_3_acc: 0.5415WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.9408 - output_1_loss: 1.5373 - output_2_loss: 2.6886 - output_3_loss: 1.9900 - output_1_acc: 0.3047 - output_2_acc: 0.2617 - output_3_acc: 0.2930\n",
            "Epoch 00018: val_loss did not improve from 2.42926\n",
            "6/6 [==============================] - 13s 2s/step - loss: 1.9219 - output_1_loss: 1.3301 - output_2_loss: 0.9188 - output_3_loss: 1.1204 - output_1_acc: 0.4504 - output_2_acc: 0.6331 - output_3_acc: 0.5449 - val_loss: 2.9408 - val_output_1_loss: 1.5373 - val_output_2_loss: 2.6886 - val_output_3_loss: 1.9900 - val_output_1_acc: 0.3047 - val_output_2_acc: 0.2617 - val_output_3_acc: 0.2930\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.09216.\n",
            "Epoch 19/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.9551 - output_1_loss: 1.3548 - output_2_loss: 0.9251 - output_3_loss: 1.0639 - output_1_acc: 0.4113 - output_2_acc: 0.6705 - output_3_acc: 0.5846WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.4634 - output_1_loss: 1.5734 - output_2_loss: 1.4922 - output_3_loss: 1.4746 - output_1_acc: 0.2109 - output_2_acc: 0.3555 - output_3_acc: 0.3750\n",
            "Epoch 00019: val_loss did not improve from 2.42926\n",
            "6/6 [==============================] - 13s 2s/step - loss: 2.0283 - output_1_loss: 1.3717 - output_2_loss: 0.9253 - output_3_loss: 1.0806 - output_1_acc: 0.4043 - output_2_acc: 0.6713 - output_3_acc: 0.5764 - val_loss: 2.4634 - val_output_1_loss: 1.5734 - val_output_2_loss: 1.4922 - val_output_3_loss: 1.4746 - val_output_1_acc: 0.2109 - val_output_2_acc: 0.3555 - val_output_3_acc: 0.3750\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.09216.\n",
            "Epoch 20/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.0545 - output_1_loss: 1.4555 - output_2_loss: 0.9843 - output_3_loss: 1.2354 - output_1_acc: 0.3228 - output_2_acc: 0.6176 - output_3_acc: 0.5065WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.4346 - output_1_loss: 1.7698 - output_2_loss: 0.9726 - output_3_loss: 1.2433 - output_1_acc: 0.1680 - output_2_acc: 0.6719 - output_3_acc: 0.4648\n",
            "Epoch 00020: val_loss did not improve from 2.42926\n",
            "6/6 [==============================] - 11s 2s/step - loss: 2.1833 - output_1_loss: 1.4702 - output_2_loss: 0.9843 - output_3_loss: 1.2441 - output_1_acc: 0.3226 - output_2_acc: 0.6195 - output_3_acc: 0.5051 - val_loss: 2.4346 - val_output_1_loss: 1.7698 - val_output_2_loss: 0.9726 - val_output_3_loss: 1.2433 - val_output_1_acc: 0.1680 - val_output_2_acc: 0.6719 - val_output_3_acc: 0.4648\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.09216.\n",
            "Epoch 21/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.2489 - output_1_loss: 1.6252 - output_2_loss: 0.9427 - output_3_loss: 1.2323 - output_1_acc: 0.2796 - output_2_acc: 0.6507 - output_3_acc: 0.5011WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.9971 - output_1_loss: 1.4148 - output_2_loss: 0.8952 - output_3_loss: 1.0458 - output_1_acc: 0.3633 - output_2_acc: 0.7109 - output_3_acc: 0.6523\n",
            "Epoch 00021: val_loss improved from 2.42926 to 1.99710, saving model to /drive/My Drive/Github/CNN/InceptionV1_best_model.h5\n",
            "6/6 [==============================] - 13s 2s/step - loss: 2.2174 - output_1_loss: 1.6110 - output_2_loss: 0.9387 - output_3_loss: 1.2263 - output_1_acc: 0.2864 - output_2_acc: 0.6490 - output_3_acc: 0.5108 - val_loss: 1.9971 - val_output_1_loss: 1.4148 - val_output_2_loss: 0.8952 - val_output_3_loss: 1.0458 - val_output_1_acc: 0.3633 - val_output_2_acc: 0.7109 - val_output_3_acc: 0.6523\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.09216.\n",
            "Epoch 22/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.9493 - output_1_loss: 1.4421 - output_2_loss: 0.8258 - output_3_loss: 1.1304 - output_1_acc: 0.3729 - output_2_acc: 0.6926 - output_3_acc: 0.5731WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.8889 - output_1_loss: 1.3432 - output_2_loss: 0.8184 - output_3_loss: 1.0005 - output_1_acc: 0.3984 - output_2_acc: 0.7031 - output_3_acc: 0.6172\n",
            "Epoch 00022: val_loss improved from 1.99710 to 1.88885, saving model to /drive/My Drive/Github/CNN/InceptionV1_best_model.h5\n",
            "6/6 [==============================] - 12s 2s/step - loss: 1.9650 - output_1_loss: 1.4324 - output_2_loss: 0.8119 - output_3_loss: 1.1158 - output_1_acc: 0.3810 - output_2_acc: 0.6967 - output_3_acc: 0.5790 - val_loss: 1.8889 - val_output_1_loss: 1.3432 - val_output_2_loss: 0.8184 - val_output_3_loss: 1.0005 - val_output_1_acc: 0.3984 - val_output_2_acc: 0.7031 - val_output_3_acc: 0.6172\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.09216.\n",
            "Epoch 23/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.8722 - output_1_loss: 1.3670 - output_2_loss: 0.7492 - output_3_loss: 0.9920 - output_1_acc: 0.4268 - output_2_acc: 0.7231 - output_3_acc: 0.5968WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.6845 - output_1_loss: 1.1331 - output_2_loss: 0.9300 - output_3_loss: 0.9080 - output_1_acc: 0.5078 - output_2_acc: 0.6133 - output_3_acc: 0.6523\n",
            "Epoch 00023: val_loss improved from 1.88885 to 1.68449, saving model to /drive/My Drive/Github/CNN/InceptionV1_best_model.h5\n",
            "6/6 [==============================] - 13s 2s/step - loss: 1.8141 - output_1_loss: 1.3455 - output_2_loss: 0.7508 - output_3_loss: 0.9903 - output_1_acc: 0.4433 - output_2_acc: 0.7241 - output_3_acc: 0.6033 - val_loss: 1.6845 - val_output_1_loss: 1.1331 - val_output_2_loss: 0.9300 - val_output_3_loss: 0.9080 - val_output_1_acc: 0.5078 - val_output_2_acc: 0.6133 - val_output_3_acc: 0.6523\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.09216.\n",
            "Epoch 24/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.8568 - output_1_loss: 1.2788 - output_2_loss: 0.7894 - output_3_loss: 1.2156 - output_1_acc: 0.4740 - output_2_acc: 0.7103 - output_3_acc: 0.5674WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.6174 - output_1_loss: 1.1412 - output_2_loss: 0.7495 - output_3_loss: 0.8377 - output_1_acc: 0.5312 - output_2_acc: 0.6797 - output_3_acc: 0.6680\n",
            "Epoch 00024: val_loss improved from 1.68449 to 1.61736, saving model to /drive/My Drive/Github/CNN/InceptionV1_best_model.h5\n",
            "6/6 [==============================] - 12s 2s/step - loss: 1.8332 - output_1_loss: 1.2800 - output_2_loss: 0.7798 - output_3_loss: 1.1773 - output_1_acc: 0.4725 - output_2_acc: 0.7140 - output_3_acc: 0.5760 - val_loss: 1.6174 - val_output_1_loss: 1.1412 - val_output_2_loss: 0.7495 - val_output_3_loss: 0.8377 - val_output_1_acc: 0.5312 - val_output_2_acc: 0.6797 - val_output_3_acc: 0.6680\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.08847359999999999.\n",
            "Epoch 25/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.6974 - output_1_loss: 1.2373 - output_2_loss: 0.7319 - output_3_loss: 0.9602 - output_1_acc: 0.5149 - output_2_acc: 0.7405 - output_3_acc: 0.6896WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.8286 - output_1_loss: 1.2131 - output_2_loss: 0.9629 - output_3_loss: 1.0889 - output_1_acc: 0.4883 - output_2_acc: 0.5859 - output_3_acc: 0.6055\n",
            "Epoch 00025: val_loss did not improve from 1.61736\n",
            "6/6 [==============================] - 12s 2s/step - loss: 1.7356 - output_1_loss: 1.2322 - output_2_loss: 0.7313 - output_3_loss: 0.9691 - output_1_acc: 0.5169 - output_2_acc: 0.7399 - output_3_acc: 0.6797 - val_loss: 1.8286 - val_output_1_loss: 1.2131 - val_output_2_loss: 0.9629 - val_output_3_loss: 1.0889 - val_output_1_acc: 0.4883 - val_output_2_acc: 0.5859 - val_output_3_acc: 0.6055\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.08847359999999999.\n",
            "Epoch 26/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.8675 - output_1_loss: 1.2336 - output_2_loss: 0.7652 - output_3_loss: 1.0937 - output_1_acc: 0.4770 - output_2_acc: 0.6939 - output_3_acc: 0.5687WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.7298 - output_1_loss: 1.2027 - output_2_loss: 0.7652 - output_3_loss: 0.9919 - output_1_acc: 0.5039 - output_2_acc: 0.7227 - output_3_acc: 0.5898\n",
            "Epoch 00026: val_loss did not improve from 1.61736\n",
            "6/6 [==============================] - 12s 2s/step - loss: 1.8709 - output_1_loss: 1.2517 - output_2_loss: 0.7692 - output_3_loss: 1.1050 - output_1_acc: 0.4706 - output_2_acc: 0.6943 - output_3_acc: 0.5603 - val_loss: 1.7298 - val_output_1_loss: 1.2027 - val_output_2_loss: 0.7652 - val_output_3_loss: 0.9919 - val_output_1_acc: 0.5039 - val_output_2_acc: 0.7227 - val_output_3_acc: 0.5898\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.08847359999999999.\n",
            "Epoch 27/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.7383 - output_1_loss: 1.2863 - output_2_loss: 0.7148 - output_3_loss: 1.0165 - output_1_acc: 0.4164 - output_2_acc: 0.7239 - output_3_acc: 0.5911WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.3928 - output_1_loss: 1.0089 - output_2_loss: 0.6232 - output_3_loss: 0.6565 - output_1_acc: 0.5273 - output_2_acc: 0.7734 - output_3_acc: 0.7461\n",
            "Epoch 00027: val_loss improved from 1.61736 to 1.39278, saving model to /drive/My Drive/Github/CNN/InceptionV1_best_model.h5\n",
            "6/6 [==============================] - 13s 2s/step - loss: 1.7249 - output_1_loss: 1.2719 - output_2_loss: 0.7068 - output_3_loss: 0.9948 - output_1_acc: 0.4201 - output_2_acc: 0.7257 - output_3_acc: 0.6002 - val_loss: 1.3928 - val_output_1_loss: 1.0089 - val_output_2_loss: 0.6232 - val_output_3_loss: 0.6565 - val_output_1_acc: 0.5273 - val_output_2_acc: 0.7734 - val_output_3_acc: 0.7461\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.08847359999999999.\n",
            "Epoch 28/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.8738 - output_1_loss: 1.4230 - output_2_loss: 0.7565 - output_3_loss: 1.0372 - output_1_acc: 0.3747 - output_2_acc: 0.7109 - output_3_acc: 0.6008WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.2422 - output_1_loss: 1.4418 - output_2_loss: 1.3233 - output_3_loss: 1.3445 - output_1_acc: 0.3906 - output_2_acc: 0.5195 - output_3_acc: 0.4883\n",
            "Epoch 00028: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 12s 2s/step - loss: 1.8235 - output_1_loss: 1.3927 - output_2_loss: 0.7439 - output_3_loss: 1.0197 - output_1_acc: 0.3834 - output_2_acc: 0.7172 - output_3_acc: 0.6047 - val_loss: 2.2422 - val_output_1_loss: 1.4418 - val_output_2_loss: 1.3233 - val_output_3_loss: 1.3445 - val_output_1_acc: 0.3906 - val_output_2_acc: 0.5195 - val_output_3_acc: 0.4883\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.08847359999999999.\n",
            "Epoch 29/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.5996 - output_1_loss: 1.1221 - output_2_loss: 0.7098 - output_3_loss: 0.8369 - output_1_acc: 0.4973 - output_2_acc: 0.7249 - output_3_acc: 0.6606WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.9779 - output_1_loss: 1.4212 - output_2_loss: 0.8981 - output_3_loss: 0.9576 - output_1_acc: 0.3789 - output_2_acc: 0.6328 - output_3_acc: 0.6055\n",
            "Epoch 00029: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 13s 2s/step - loss: 1.5770 - output_1_loss: 1.1217 - output_2_loss: 0.7034 - output_3_loss: 0.8358 - output_1_acc: 0.5070 - output_2_acc: 0.7268 - output_3_acc: 0.6664 - val_loss: 1.9779 - val_output_1_loss: 1.4212 - val_output_2_loss: 0.8981 - val_output_3_loss: 0.9576 - val_output_1_acc: 0.3789 - val_output_2_acc: 0.6328 - val_output_3_acc: 0.6055\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.08847359999999999.\n",
            "Epoch 30/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.2886 - output_1_loss: 1.3312 - output_2_loss: 0.6809 - output_3_loss: 0.9316 - output_1_acc: 0.5045 - output_2_acc: 0.7603 - output_3_acc: 0.6665WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.7142 - output_1_loss: 1.8811 - output_2_loss: 1.1187 - output_3_loss: 1.6582 - output_1_acc: 0.1641 - output_2_acc: 0.5781 - output_3_acc: 0.2344\n",
            "Epoch 00030: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 11s 2s/step - loss: 2.3445 - output_1_loss: 1.4333 - output_2_loss: 0.7610 - output_3_loss: 1.0223 - output_1_acc: 0.4734 - output_2_acc: 0.7319 - output_3_acc: 0.6310 - val_loss: 2.7142 - val_output_1_loss: 1.8811 - val_output_2_loss: 1.1187 - val_output_3_loss: 1.6582 - val_output_1_acc: 0.1641 - val_output_2_acc: 0.5781 - val_output_3_acc: 0.2344\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.08847359999999999.\n",
            "Epoch 31/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.4795 - output_1_loss: 1.7760 - output_2_loss: 1.1587 - output_3_loss: 1.5127 - output_1_acc: 0.1889 - output_2_acc: 0.5560 - output_3_acc: 0.3284WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.3246 - output_1_loss: 1.6353 - output_2_loss: 0.8237 - output_3_loss: 1.4741 - output_1_acc: 0.2188 - output_2_acc: 0.7070 - output_3_acc: 0.4375\n",
            "Epoch 00031: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.4598 - output_1_loss: 1.7543 - output_2_loss: 1.1372 - output_3_loss: 1.4947 - output_1_acc: 0.1896 - output_2_acc: 0.5646 - output_3_acc: 0.3383 - val_loss: 2.3246 - val_output_1_loss: 1.6353 - val_output_2_loss: 0.8237 - val_output_3_loss: 1.4741 - val_output_1_acc: 0.2188 - val_output_2_acc: 0.7070 - val_output_3_acc: 0.4375\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.08847359999999999.\n",
            "Epoch 32/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.2745 - output_1_loss: 1.6055 - output_2_loss: 1.0022 - output_3_loss: 1.3319 - output_1_acc: 0.2317 - output_2_acc: 0.6130 - output_3_acc: 0.4565WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.1808 - output_1_loss: 1.6542 - output_2_loss: 0.7101 - output_3_loss: 1.0455 - output_1_acc: 0.1680 - output_2_acc: 0.7148 - output_3_acc: 0.6445\n",
            "Epoch 00032: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.2643 - output_1_loss: 1.6052 - output_2_loss: 0.9864 - output_3_loss: 1.3094 - output_1_acc: 0.2273 - output_2_acc: 0.6190 - output_3_acc: 0.4693 - val_loss: 2.1808 - val_output_1_loss: 1.6542 - val_output_2_loss: 0.7101 - val_output_3_loss: 1.0455 - val_output_1_acc: 0.1680 - val_output_2_acc: 0.7148 - val_output_3_acc: 0.6445\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.084934656.\n",
            "Epoch 33/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.2336 - output_1_loss: 1.6064 - output_2_loss: 0.9729 - output_3_loss: 1.3201 - output_1_acc: 0.2077 - output_2_acc: 0.6260 - output_3_acc: 0.5092WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.2767 - output_1_loss: 1.7108 - output_2_loss: 0.7292 - output_3_loss: 1.1574 - output_1_acc: 0.2266 - output_2_acc: 0.7227 - output_3_acc: 0.6016\n",
            "Epoch 00033: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.2321 - output_1_loss: 1.6041 - output_2_loss: 0.9563 - output_3_loss: 1.2851 - output_1_acc: 0.2101 - output_2_acc: 0.6376 - output_3_acc: 0.5252 - val_loss: 2.2767 - val_output_1_loss: 1.7108 - val_output_2_loss: 0.7292 - val_output_3_loss: 1.1574 - val_output_1_acc: 0.2266 - val_output_2_acc: 0.7227 - val_output_3_acc: 0.6016\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.084934656.\n",
            "Epoch 34/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.1576 - output_1_loss: 1.5844 - output_2_loss: 0.9410 - output_3_loss: 1.2236 - output_1_acc: 0.3169 - output_2_acc: 0.6782 - output_3_acc: 0.5633WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.0010 - output_1_loss: 1.5546 - output_2_loss: 0.7003 - output_3_loss: 0.7875 - output_1_acc: 0.3672 - output_2_acc: 0.7188 - output_3_acc: 0.7227\n",
            "Epoch 00034: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.1427 - output_1_loss: 1.5800 - output_2_loss: 0.9020 - output_3_loss: 1.1929 - output_1_acc: 0.3127 - output_2_acc: 0.6919 - output_3_acc: 0.5718 - val_loss: 2.0010 - val_output_1_loss: 1.5546 - val_output_2_loss: 0.7003 - val_output_3_loss: 0.7875 - val_output_1_acc: 0.3672 - val_output_2_acc: 0.7188 - val_output_3_acc: 0.7227\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.084934656.\n",
            "Epoch 35/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.0506 - output_1_loss: 1.5494 - output_2_loss: 0.7779 - output_3_loss: 1.0525 - output_1_acc: 0.3214 - output_2_acc: 0.7194 - output_3_acc: 0.6271WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.8989 - output_1_loss: 1.4866 - output_2_loss: 0.6510 - output_3_loss: 0.7233 - output_1_acc: 0.3750 - output_2_acc: 0.7539 - output_3_acc: 0.7344\n",
            "Epoch 00035: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.0398 - output_1_loss: 1.5410 - output_2_loss: 0.7623 - output_3_loss: 1.0402 - output_1_acc: 0.3285 - output_2_acc: 0.7263 - output_3_acc: 0.6347 - val_loss: 1.8989 - val_output_1_loss: 1.4866 - val_output_2_loss: 0.6510 - val_output_3_loss: 0.7233 - val_output_1_acc: 0.3750 - val_output_2_acc: 0.7539 - val_output_3_acc: 0.7344\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.084934656.\n",
            "Epoch 36/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.0691 - output_1_loss: 1.4715 - output_2_loss: 0.6757 - output_3_loss: 0.9177 - output_1_acc: 0.4085 - output_2_acc: 0.7405 - output_3_acc: 0.6620WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 2.1683 - output_1_loss: 1.6127 - output_2_loss: 0.7631 - output_3_loss: 1.0889 - output_1_acc: 0.2539 - output_2_acc: 0.7031 - output_3_acc: 0.5508\n",
            "Epoch 00036: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 12s 2s/step - loss: 2.1202 - output_1_loss: 1.5023 - output_2_loss: 0.6906 - output_3_loss: 0.9626 - output_1_acc: 0.3859 - output_2_acc: 0.7391 - output_3_acc: 0.6376 - val_loss: 2.1683 - val_output_1_loss: 1.6127 - val_output_2_loss: 0.7631 - val_output_3_loss: 1.0889 - val_output_1_acc: 0.2539 - val_output_2_acc: 0.7031 - val_output_3_acc: 0.5508\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.084934656.\n",
            "Epoch 37/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 2.1074 - output_1_loss: 1.6030 - output_2_loss: 0.7339 - output_3_loss: 1.1728 - output_1_acc: 0.2443 - output_2_acc: 0.7085 - output_3_acc: 0.4814WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.8287 - output_1_loss: 1.3436 - output_2_loss: 0.6593 - output_3_loss: 0.9575 - output_1_acc: 0.4023 - output_2_acc: 0.7383 - output_3_acc: 0.6094\n",
            "Epoch 00037: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 11s 2s/step - loss: 2.0851 - output_1_loss: 1.5858 - output_2_loss: 0.7217 - output_3_loss: 1.1565 - output_1_acc: 0.2584 - output_2_acc: 0.7167 - output_3_acc: 0.4923 - val_loss: 1.8287 - val_output_1_loss: 1.3436 - val_output_2_loss: 0.6593 - val_output_3_loss: 0.9575 - val_output_1_acc: 0.4023 - val_output_2_acc: 0.7383 - val_output_3_acc: 0.6094\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.084934656.\n",
            "Epoch 38/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.8859 - output_1_loss: 1.3687 - output_2_loss: 0.6545 - output_3_loss: 1.0082 - output_1_acc: 0.3727 - output_2_acc: 0.7510 - output_3_acc: 0.6110WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.7168 - output_1_loss: 1.2908 - output_2_loss: 0.6231 - output_3_loss: 0.7970 - output_1_acc: 0.4688 - output_2_acc: 0.7422 - output_3_acc: 0.6953\n",
            "Epoch 00038: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 13s 2s/step - loss: 1.8574 - output_1_loss: 1.3733 - output_2_loss: 0.6404 - output_3_loss: 0.9973 - output_1_acc: 0.3700 - output_2_acc: 0.7596 - output_3_acc: 0.6169 - val_loss: 1.7168 - val_output_1_loss: 1.2908 - val_output_2_loss: 0.6231 - val_output_3_loss: 0.7970 - val_output_1_acc: 0.4688 - val_output_2_acc: 0.7422 - val_output_3_acc: 0.6953\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.084934656.\n",
            "Epoch 39/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.7987 - output_1_loss: 1.3556 - output_2_loss: 0.5645 - output_3_loss: 0.8442 - output_1_acc: 0.3900 - output_2_acc: 0.7945 - output_3_acc: 0.6787WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.6724 - output_1_loss: 1.3057 - output_2_loss: 0.4633 - output_3_loss: 0.7594 - output_1_acc: 0.4531 - output_2_acc: 0.8359 - output_3_acc: 0.6992\n",
            "Epoch 00039: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 13s 2s/step - loss: 1.8015 - output_1_loss: 1.3569 - output_2_loss: 0.5739 - output_3_loss: 0.8527 - output_1_acc: 0.3902 - output_2_acc: 0.7918 - output_3_acc: 0.6752 - val_loss: 1.6724 - val_output_1_loss: 1.3057 - val_output_2_loss: 0.4633 - val_output_3_loss: 0.7594 - val_output_1_acc: 0.4531 - val_output_2_acc: 0.8359 - val_output_3_acc: 0.6992\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.084934656.\n",
            "Epoch 40/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.7554 - output_1_loss: 1.3510 - output_2_loss: 0.5489 - output_3_loss: 0.8292 - output_1_acc: 0.4098 - output_2_acc: 0.8082 - output_3_acc: 0.6741WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "Epoch 1/1000\n",
            "2/6 [=========>....................] - ETA: 4s - loss: 1.6428 - output_1_loss: 1.2890 - output_2_loss: 0.4723 - output_3_loss: 0.7070 - output_1_acc: 0.4492 - output_2_acc: 0.8281 - output_3_acc: 0.7305\n",
            "Epoch 00040: val_loss did not improve from 1.39278\n",
            "6/6 [==============================] - 11s 2s/step - loss: 1.7724 - output_1_loss: 1.3561 - output_2_loss: 0.5421 - output_3_loss: 0.8249 - output_1_acc: 0.4033 - output_2_acc: 0.8114 - output_3_acc: 0.6785 - val_loss: 1.6428 - val_output_1_loss: 1.2890 - val_output_2_loss: 0.4723 - val_output_3_loss: 0.7070 - val_output_1_acc: 0.4492 - val_output_2_acc: 0.8281 - val_output_3_acc: 0.7305\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.08153726975999999.\n",
            "Epoch 41/1000\n",
            "5/6 [========================>.....] - ETA: 1s - loss: 1.7283 - output_1_loss: 1.3492 - output_2_loss: 0.4893 - output_3_loss: 0.7921 - output_1_acc: 0.4043 - output_2_acc: 0.8343 - output_3_acc: 0.7031"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Keras_worker_ForkPoolWorker-1:\n",
            "Process Keras_worker_ForkPoolWorker-41:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/utils/data_utils.py\", line 924, in next_sample\n",
            "    return six.next(_SHARED_SEQUENCES[uid])\n",
            "  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/utils/data_utils.py\", line 924, in next_sample\n",
            "    return six.next(_SHARED_SEQUENCES[uid])\n",
            "  File \"<ipython-input-65-cb55625e4389>\", line 23, in my_generator\n",
            "    data = next(generator)\n",
            "  File \"<ipython-input-65-cb55625e4389>\", line 23, in my_generator\n",
            "    data = next(generator)\n",
            "  File \"/tensorflow-2.1.0/python3.6/keras_preprocessing/image/iterator.py\", line 104, in __next__\n",
            "    return self.next(*args, **kwargs)\n",
            "  File \"/tensorflow-2.1.0/python3.6/keras_preprocessing/image/iterator.py\", line 104, in __next__\n",
            "    return self.next(*args, **kwargs)\n",
            "  File \"/tensorflow-2.1.0/python3.6/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/tensorflow-2.1.0/python3.6/keras_preprocessing/image/iterator.py\", line 116, in next\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/tensorflow-2.1.0/python3.6/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/tensorflow-2.1.0/python3.6/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/tensorflow-2.1.0/python3.6/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/tensorflow-2.1.0/python3.6/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-81da00131b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m              metrics=['accuracy'])\n\u001b[1;32m     20\u001b[0m history = model.fit(final_generator, epochs=epochs, validation_data=val_generator, steps_per_epoch=6, validation_steps=2,\n\u001b[0;32m---> 21\u001b[0;31m                        verbose=1, callbacks=callbacks_list, use_multiprocessing=True)\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3tuN8aGkma6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AknaRcLkma8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}